
```{python}
#| label: set-up
import os
import numpy as np
import pandas as pd
import sklearn as sk
import networkx as nx
import matplotlib.pyplot as plt
import shap
import os
import sys
from datetime import datetime
from sklearn.ensemble import IsolationForest

sys.path.append(os.getcwd())
sys.path.append(os.path.dirname(os.getcwd()))

import scripts.cons as cons
```

# Load Random Telecom Payments Data

```{python}
#| label: data-load
# load random telecom payments data
data_fpath=os.path.join('..', 'data', 'arch', 'RandomTelecomPaymentsV1.1.csv')
parse_dates = ['registration_date', 'transaction_date']
date_parser = lambda x: datetime.strptime(x, '%Y-%m-%d')
data = pd.read_csv(filepath_or_buffer=data_fpath,  parse_dates=parse_dates)

# determine the week number for all transaction dates
data['transaction_week'] = data['transaction_date'].dt.isocalendar().week
```

# Data Preparation

```{python}
#| label: data-preparation

def gen_entity_network_data(data, entity, userid = 'userid', trans_week = 'transaction_week'):
    """
    """
    # extract out the unique userids and device hashes
    user_entity_data = data[[userid, entity, trans_week]].dropna().drop_duplicates()
    # inner join users to users based on shared device hash
    user_entity_network_data = pd.merge(left = user_entity_data, right = user_entity_data, on = [entity, trans_week], how = 'inner')
    # drop rows where userid_x = userid_y
    user_entity_network_data = user_entity_network_data.loc[user_entity_network_data[f'{userid}_x'] != user_entity_network_data[f'{userid}_y'], :]
    # set col order
    col_order = [f'{userid}_x', f'{userid}_y', entity, trans_week]
    user_entity_network_data = user_entity_network_data[col_order]
    return user_entity_data, user_entity_network_data

def gen_base_network_data(entity_networks, user_cols=['userid_x','userid_y']):
    """
    """
    # create a base data of users from all entity networks
    base_data = pd.concat(objs=[df[user_cols] for df in entity_networks], ignore_index=True, axis=0).drop_duplicates().reset_index(drop=True)
    # generate full base data by joining on all entity networks
    for entity_network in entity_networks:
        base_data = pd.merge(left=base_data, right=entity_network, on=user_cols, how='left')
    return base_data

# generate share user entity networks
user_device_data, user_device_network_data = gen_entity_network_data(data=data, entity='device_hash', userid = 'userid')
user_ip_data, user_ip_network_data = gen_entity_network_data(data=data, entity='ip_hash', userid = 'userid')
user_card_data, user_card_network_data = gen_entity_network_data(data=data, entity='card_hash', userid = 'userid')
# generate base entity network data
entity_networks = [user_device_network_data, user_ip_network_data, user_card_network_data]
base_data = gen_base_network_data(entity_networks=entity_networks, user_cols=['userid_x','userid_y'])
```

# Network Analysis

```{python}
#| label: network-analysis

def gen_comp_data(network_data, entity_data, edge_attr):
    """
    """
    # apply graphs for each week
    trans_week_graphs = network_data.groupby(by='transaction_week').apply(lambda group: nx.from_pandas_edgelist(df = group, source = 'userid_x', target = 'userid_y', edge_attr = [edge_attr])).rename('G').reset_index()
    # extract connected components for each week
    trans_week_comps = trans_week_graphs.apply(lambda series: pd.DataFrame([{'transaction_week':series['transaction_week'], 'compid':i, 'userid':cc} for i, cc in enumerate(nx.connected_components(series['G']))]).explode('userid').reset_index(drop = True), axis=1).to_list()
    trans_week_comps = pd.concat(trans_week_comps, axis=0)
    # calculate compid sizes across each week
    trans_week_comps_size = trans_week_comps.groupby(by = ['transaction_week', 'compid'], as_index = False).agg({'userid':'nunique'}).rename(columns={'userid':'compsize'})
    # generate the component data
    comp_data = pd.merge(left = entity_data, right = trans_week_comps, left_on = ['transaction_week', 'userid'], right_on = ['transaction_week', 'userid'], how = 'inner')
    comp_data = pd.merge(left = comp_data, right = trans_week_comps_size, on = ['transaction_week', 'compid'], how = 'inner')
    # order by comp size
    comp_data = comp_data.sort_values(by = ['transaction_week', 'compid', 'userid', edge_attr]).reset_index(drop=True)
    # normalise data with respect to edge attribute
    comp_data = comp_data.rename(columns={edge_attr:'idhashes'})
    comp_data['type'] = edge_attr
    return comp_data

# generate components for all entities
user_device_comp_data = gen_comp_data(network_data=user_device_network_data, entity_data = user_device_data, edge_attr='device_hash')
user_ip_comp_data = gen_comp_data(network_data=user_ip_network_data, entity_data = user_ip_data, edge_attr='ip_hash')
user_card_comp_data = gen_comp_data(network_data=user_card_network_data, entity_data = user_card_data, edge_attr='card_hash')
# concatenate component data together
concat_objs = [user_device_comp_data, user_ip_comp_data, user_card_comp_data]
user_entity_comp_data = pd.concat(objs=concat_objs, axis=0, ignore_index=True)
```

# Graph Connected Component

```{python}
#| label: connected-component
type_filter = user_entity_comp_data['type'].isin(['device_hash', 'card_hash', 'ip_hash'])
trans_week_filter = user_entity_comp_data['transaction_week'].isin([52])
comp_data = user_entity_comp_data.loc[type_filter & trans_week_filter, :].copy()

# generate grpah from component data
compids = comp_data['compid'].unique()
comp_data = comp_data.loc[comp_data['compid'].isin(compids), :]
G = nx.from_pandas_edgelist(df = comp_data, source = 'userid', target = 'idhashes', edge_attr = ['compid', 'compsize', 'type'])

# define node colours
idhash_colours = comp_data.drop_duplicates(subset=['idhashes', 'type']).set_index('idhashes')['type'].replace({'device_hash':'orange', 'ip_hash':'yellow', 'card_hash':'red'})
userid_colours = comp_data.drop_duplicates(subset=['userid']).set_index('userid').assign(colour='blue')['colour']
colours_df = pd.concat(objs = [userid_colours, idhash_colours], axis=0).rename('colours').to_frame().reset_index().rename(columns={'index':'nodes'})
# join color to nodes
node_colours_df = pd.Series(list(G), name='nodes').to_frame().merge(colours_df, on='nodes', how='left')

# plot network
fig, ax = plt.subplots()
nx.draw_networkx(G, pos = nx.spring_layout(G), with_labels = True, node_size = 30, font_size = 1, node_color=node_colours_df['colours'].to_list(), ax=ax)
ax.set(title="Connected Component")
plt.show()
```

# Feature Data

```{python}
# generate user network feature data
groupby_cols = ['userid', 'transaction_week']
agg_dict = {'type':'nunique', 'compsize':'sum'}
rename_dict = {'type':'n_comps', 'compsize':'total_comp_size'}
feat_data = user_entity_comp_data.groupby(by=groupby_cols, as_index=False).agg(agg_dict).rename(columns=rename_dict)
# sort by n comps
feat_data = feat_data.sort_values(by=['transaction_week', 'userid']).reset_index(drop=True)
```

# Write Data

```{python}
#| label: write data
feat_data_fpath=os.path.join('..', 'data', 'report', 'user_comp_data.csv')
feat_data.to_csv(feat_data_fpath, index=False)
```